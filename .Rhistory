sum(knn.model1==factorclassificador)/length(factorclassificador)
sum(knn.model3==factorclassificador)/length(factorclassificador)
sum(knn.model5==factorclassificador)/length(factorclassificador)
rf
rf
rf$error
rf$err.rate
rf$err.rate[1]
rf$err.rate[100]
rf$err.rate[100,]
rf$err.rate[100]
rf$err.rate[100]
cross(svpVanillaDot);cross(svpPolyDot);cross(svpTanhDot)
svpVanillaDot
svpVanillaDot
svpVanillaDot
ypred = predict(svpVanillaDot,testing)
ypred
table(colon.y[-intrain],ypred)
table(ypred,(colon.y[-intrain])
table(ypred,(colon.y[-intrain]))
table(ypred,(colon.y[-intrain]))
svpTanhDot <- ksvm(as.matrix(training),factorclassificador,type="C-svc",kernel='tanhdot',C=1,scaled=c(),cross=5)
svpPolyDot <- ksvm(as.matrix(training),factorclassificador,type="C-svc",kernel='polydot',C=1,scaled=c(),cross=5)
svpTanhDot
ypred = predict(svpTanhDot,testing)
table(ypred,(colon.y[-intrain]))
set.seed(13039)
table(colon.y)
samp1<-sample(1:22, 15)
samp2<-sample(23:62, 27)
train<- c(samp1,samp2)
colon.x<-cbind(colon.y, colon.x)
#head(colon.x)
myDataTrain<-colon.x[train,]
myDataTest<-colon.x[-train,]
knn.model <- knn(train=myDataTrain[,-1], test=myDataTest[,-1], cl= myDataTrain[,1], k=7)
table(knn.model, myDataTest[,1])
table(knn.model, colon.y[-intrain])
knn.model3 <- knn(train=training, test=testing, cl= factorclassificador, k=3)
table(knn.model, colon.y[-intrain])
knn.model1 <- knn(train=training, test=testing, cl= factorclassificador, k=1)
table(knn.model1, colon.y[-intrain])
knn.model3 <- knn(train=training, test=testing, cl= factorclassificador, k=3)
table(knn.model3, colon.y[-intrain])
#k=5
knn.model5 <- knn(train=training, test=testing, cl= factorclassificador, k=5)
table(knn.model5, colon.y[-intrain])
set.seed(12345)
intrain <- createDataPartition(colon.y, p= 0.75, list=FALSE)
training <- colon[intrain,]
testing <- colon[-intrain,]
library(class)
factorclassificador <- colon.y[intrain]
#k=1
knn.model1 <- knn(train=training, test=testing, cl= factorclassificador, k=1)
table(knn.model1, colon.y[-intrain])
#k=3
knn.model3 <- knn(train=training, test=testing, cl= factorclassificador, k=3)
table(knn.model3, colon.y[-intrain])
#k=5
knn.model5 <- knn(train=training, test=testing, cl= factorclassificador, k=5)
table(knn.model5, colon.y[-intrain])
pred_knn1 <- table(knn.model1, testing)
pred_knn1 <- table(knn.model1, colon.y[-train])
table(knn.model1, testing)
pred_knn1 <- table(knn.model1, colon.y[-train])
knn.model <- knn(train=myDataTrain[,-1], test=myDataTest[,-1], cl= myDataTrain[,1], k=7)
str(knn.model)
length(knn.model)
table(knn.model, myDataTest[,1])
(pred_knn <- table(knn.model, colon.y[-train]))
#k=3
knn.model <- knn(train=myDataTrain[,-1], test=myDataTest[,-1], cl= myDataTrain[,1], k=3)
str(knn.model)
length(knn.model)
table(knn.model, myDataTest[,1])
(pred_knn <- table(knn.model, colon.y[-train]))
#k=5
knn.model <- knn(train=myDataTrain[,-1], test=myDataTest[,-1], cl= myDataTrain[,1], k=5)
str(knn.model)
length(knn.model)
table(knn.model, myDataTest[,1])
(pred_knn <- table(knn.model, colon.y[-train]))
table(ypred,(colon.y[-intrain]))
table(knn.model1, colon.y[-intrain])
knn.model3 <- knn(train=training, test=testing, cl= factorclassificador, k=3)
table(knn.model3, colon.y[-intrain])
#k=5
knn.model5 <- knn(train=training, test=testing, cl= factorclassificador, k=5)
table(knn.model5, colon.y[-intrain])
table(knn.model1, colon[intrain])
table(knn.model1, colon.y[-intrain])
(k1<- table(knn.model1, colon.y[-intrain]))
(k2<-table(knn.model3, colon.y[-intrain]))
(k3<-table(knn.model5, colon.y[-intrain]))
sum(k1[1,1],k[2,2])/length(colon.y[-intrain])
sum(k1[1,1],k1[2,2])/length(colon.y[-intrain])
sum(knn.model3==factorclassificador)/length(factorclassificador)
sum(k2[1,1],k2[2,2])/length(colon.y[-intrain])
sum(k3[1,1],k3[2,2])/length(colon.y[-intrain])
cross(svpVanillaDot);cross(svpPolyDot);cross(svpTanhDot)
sum(ypred==colon.y[-train])/length(colon.y[-train])
sum(ypred==myDataTest[,1])/length(myDataTest[,1])
sum(ypred==colon.y[-intrain])/length(colon.y[-intrain])
library(kernlab)
#c=1
svpVanillaDot <- ksvm(as.matrix(training),factorclassificador,type="C-svc",kernel='vanilladot',C=1,scaled=c(),cross=5)
ypred = predict(svpVanillaDot,testing)
# Confusion table
table(ypred,(colon.y[-intrain]))
sum(ypred==colon.y[-intrain])/length(colon.y[-intrain])
alpha(svpVanillaDot)
alphaindex(svpVanillaDot)
nSV(svpVanillaDot)
b(svpVanillaDot)
ypred = predict(svpVanillaDot2,testing)
table(ypred,(colon.y[-intrain]))
table(ypred,(colon.y[-intrain]))
alpha(svpVanillaDot2)
alphaindex(svpVanillaDot2)
nSV(svpVanillaDot2)
b(svpVanillaDot2)
sum(ypred==colon.y[-intrain])/length(colon.y[-intrain])
fit.lasso <- glmnet(x = as.matrix(training), y = factorclassificador, family="gaussian", alpha=0)
library(glmnet)
#alpha=0
fit.lasso <- glmnet(x = as.matrix(training), y = factorclassificador, family="gaussian", alpha=0)
print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=0")
plot(fit.lasso, xvar="lambda",label=T, main="alpha=0")
#alpha=0.5
fit.lasso2 <- glmnet(x = as.matrix(training), y = factorclassificador, family="gaussian", alpha=0.5)
#print(fit.lasso2)
plot(fit.lasso2, label=T, main="alpha=0.5")
plot(fit.lasso2, xvar="lambda",label=T, main="alpha=0.5")
#alpha=1
fit.lasso3 <- glmnet(x = as.matrix(training), y = factorclassificador, family="gaussian", alpha=1)
plot(fit.lasso3, label=T, main="alpha=1")
plot(fit.lasso3, xvar="lambda",label=T, main="alpha=1")
print(fit.lasso3)
plot(fit.lasso3, label=T, main="alpha=1")
plot(fit.lasso3, xvar="lambda",label=T, main="alpha=1")
1-rf$err.rate[100]
sum(pred.rf[1,1],pred.rf[2,2])/length(colon.y[-intrain])
pred.rf<-table(rfpredcit, testing$colon.y)
sum(pred.rf[1,1],pred.rf[2,2])/length(colon.y[-intrain])
library(glmnet)
#alpha=0
fit.lasso <- glmnet(x = as.matrix(myDataTrain[,-1]), y = myDataTrain[,1], family="gaussian", alpha=0)
#print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=0")
plot(fit.lasso, xvar="lambda",label=T, main="alpha=0") #240
#alpha=0.5
fit.lasso <- glmnet(x = as.matrix(myDataTrain[,-1]), y = myDataTrain[,1], family="gaussian", alpha=0.5)
#print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=0.5")
plot(fit.lasso, xvar="lambda",label=T, main="alpha=0.5")#175
#alpha=1
fit.lasso <- glmnet(x = as.matrix(myDataTrain[,-1]), y = myDataTrain[,1], family="gaussian", alpha=1)
#print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=1")
plot(fit.lasso, xvar="lambda",label=T, main="alpha=1")#175
plot(fit.lasso, xvar="lambda",label=T, main="alpha=0") #240
fit.lasso <- glmnet(x = as.matrix(training[,-1]), y = training[,1], family="gaussian", alpha=0)
print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=0")
plot(fit.lasso, xvar="lambda",label=T, main="alpha=0")
fit.lasso2 <- glmnet(x = as.matrix(training[,-1]), y = training[,1], family="gaussian", alpha=0.5)
plot(fit.lasso2, label=T, main="alpha=0.5")
plot(fit.lasso2, xvar="lambda",label=T, main="alpha=0.5")
fit.lasso3 <- glmnet(x = as.matrix(training[,-1]), y = training[,1], family="gaussian", alpha=1)
plot(fit.lasso3, label=T, main="alpha=1")
plot(fit.lasso3, xvar="lambda",label=T, main="alpha=1")
print(fit.lasso)
fit.lasso <- glmnet(x = as.matrix(myDataTrain[,-1]), y = myDataTrain[,1], family="gaussian", alpha=0)
#print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=0")
plot(fit.lasso, xvar="lambda",label=T, main="alpha=0") #240
fit.lasso
plot(fit.lasso, label=T, main="alpha=0")
fit.lasso2 <- glmnet(x = as.matrix(training[,-1]), y = training[,1], family="gaussian", alpha=0.5)
#print(fit.lasso2)
plot(fit.lasso2, label=T, main="alpha=0.5")
fit.lasso3 <- glmnet(x = as.matrix(training[,-1]), y = training[,1], family="gaussian", alpha=1)
#print(fit.lasso3)
plot(fit.lasso3, label=T, main="alpha=1")
library(glmnet)
#alpha=0
fit.lasso <- glmnet(x = as.matrix(myDataTrain[,-1]), y = myDataTrain[,1], family="gaussian", alpha=0)
#print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=0")
fit.lasso <- glmnet(x = as.matrix(myDataTrain[,-1]), y = myDataTrain[,1], family="gaussian", alpha=0.5)
#print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=0.5")
library(glmnet)
#alpha=0
fit.lasso <- glmnet(x = as.matrix(training[,-1]), y = training[,1], family="gaussian", alpha=0)
print(fit.lasso)
plot(fit.lasso, label=T, main="alpha=0")
plot(fit.lasso, xvar="lambda",label=T, main="alpha=0")
#Variable mÃ¡s importante: 240
#alpha=0.5
fit.lasso2 <- glmnet(x = as.matrix(training[,-1]), y = training[,1], family="gaussian", alpha=0.5)
#print(fit.lasso2)
plot(fit.lasso2, label=T, main="alpha=0.5")
varImpPlot(rf)
library(randomForest)
training
colon.y_fact<-as.factor(colon.y)
colon <- data.frame(colon.x,colon.y_fact)
colon[train,]
training
147*2+165 - 105*2 + 141*2 -101 - 105*2
101+2*130+147-101-101*2-105
sqrt(50^2+50^2)
sqrt(50^2-50^2)
load("~/Google Drive/TFM (Xavi y Jorge)/Jorge/TFM/resultados1/SessionDataSetNewVariables.RData")
View(dataset_util)
library(caret)
inTrain <- createDataPartition(y= dataset_util$Etiqueta,
p= 0.7, list = FALSE )
training <- dataset_util[inTrain,]
testing <- dataset_util[-inTrain,]
modFit <- train(Etiqueta ~., data = training, method = 'rf', prox = T)
object.size(dataset_util)/1024/1024
modFit <- train(Etiqueta ~ xnorm_num0 + xtrend_num0 +
xint_num0 + Var_res_num0, data = training,
method = 'rf', prox = T)
method = 'rf', prox = F)
modFit <- train(Etiqueta ~ xnorm_num0 + xtrend_num0 +
xint_num0 + Var_res_num0, data = training,
method = 'rf', prox = F)
subset_dataset_util = subset(dataset_util, 1000)
subset_dataset_util_reducido = sample(dataset_util, size = 1000, replace = F)
subset_dataset_util_reducido = sample(dataset_util, size = 500, replace = F)
nros(dataset_util)
nrow(dataset_util)
subset_dataset_util_reducido = sample(nrows(dataset_util), size = 1000, replace = F)
subset_dataset_util_reducido = sample(nrow(dataset_util), size = 1000, replace = F)
rm(subset_dataset_util_reducido)
subset_dataset_util_reducido = dataset_util[sample(nrow(dataset_util),1000),]
training <- subset_dataset_util_reducido[inTrain,]
testing <- subset_dataset_util_reducido[-inTrain,]
modFit <- train(Etiqueta ~., data = training, method = 'rf', prox = T)
modFit
summary(modFit)
pred <- predict(modFit,testing)
pred
testing$predrigth <- pred ==testing$Etiqueta
table(pred,testing$Etiqueta)
modFit
summary(modFit)
modFit$finalModel
confusionMatrix(pred,testing$Etiqueta)
5/18
modFit <- train(Etiqueta ~., data = training, method = 'lda', prox = T)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
table(subset_dataset_util_reducido$Etiqueta)
confusionMatrix(pred,testing$Etiqueta)
modFit <- train(Etiqueta ~., data = training, method = 'bstTree', prox = T)
modFit <- train(Etiqueta ~., data = training, method = 'lda', prox = T)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
varImpPlot(modFit)
plot(modFit)
modFit <- train(Etiqueta ~., data = training, method = 'bayesglm', prox = T)
modFit <- train(Etiqueta ~., data = training, method = 'binda', prox = T)
modFit <- train(Etiqueta ~., data = training, method = 'rpart', prox = T)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
modFit <- train(Etiqueta ~., data = training, method = 'fda', prox = T)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
modFit <- train(Etiqueta ~., data = training, method = 'loclda', prox = T, k= 5)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
modFit
modFit <- train(Etiqueta ~., data = training, method = 'rf', prox = T, k= 5)
modFit <- train(Etiqueta ~., data = training, method = 'rf', prox = F)
subset_dataset_util_reducido = dataset_util[sample(nrow(dataset_util),8000),]
training <- subset_dataset_util_reducido[inTrain,]
testing <- subset_dataset_util_reducido[-inTrain,]
modFit <- train(Etiqueta ~., data = training, method = 'lda', prox = F)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
modFit <- train(Etiqueta ~., data = training, method = 'qda', prox = F)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
modFit <- train(Etiqueta ~., data = training, method = 'rf', prox = F)
modFit <- train(Etiqueta ~., data = training, method = 'rpart', prox = F)
modFit <- train(Etiqueta ~., data = training, method = 'rpart')
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
modFit <- train(Etiqueta ~., data = training, method = 'nb')
modFit <- train(Etiqueta ~., data = training, method = 'gbm',verbose = FALSE)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
subset_dataset_util_reducido = dataset_util[sample(nrow(dataset_util),1200),]
training <- subset_dataset_util_reducido[inTrain,]
testing <- subset_dataset_util_reducido[-inTrain,]
subset_dataset_util_reducido = dataset_util[sample(nrow(dataset_util),12000),]
training <- subset_dataset_util_reducido[inTrain,]
testing <- subset_dataset_util_reducido[-inTrain,]
modFit <- train(Etiqueta ~., data = training, method = 'gbm',verbose = FALSE)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
subset_dataset_util_reducido = dataset_util[sample(nrow(dataset_util),18000),]
training <- subset_dataset_util_reducido[inTrain,]
testing <- subset_dataset_util_reducido[-inTrain,]
modFit <- train(Etiqueta ~., data = training, method = 'gbm',verbose = FALSE)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
training <- dataset_util[inTrain,]
testing <- dataset_util[-inTrain,]
modFit <- train(Etiqueta ~., data = training, method = 'gbm',verbose = FALSE)
pred <- predict(modFit,testing)
confusionMatrix(pred,testing$Etiqueta)
library(FactoMineR)
View(dataset_util)
prComp <- PCA(dataset_util[,-6])
prComp
prComp$eig
prComp
prComp$var$cor
prComp
prComp$var
hist(dataset_util$xnorm_num0)
hist(dataset_util$xtrend_num2)
hist(dataset_util$xnorm_num1)
str <- c("Regular", "expression", "examples of R language")
str
x <- grep("ex",str,value=F)
x
str <- c("Regular", "expression", "examples of R language", "examples")
x <- grep("ex",str,value=F)
x
str <- c("Regular", "expression", "examples of R language", "examples")
x <- grep("ex",str,value=T)
x
str <- c("Regular", "expression", "examples of R language")
x <- sub("x.ress","",str)
x
str <- c("Regular", "expression", "examples of R language")
x <- sub("ress","",str)
x
iris[,1:3]
month.name
month.name[grep("a", month.name)]
gsub("(i.*a)", "xxx_\\1", "virginica", perl = TRUE)
gsub("(i.*)", "xxx_\\1", "virginica", perl = TRUE)
gsub("(.*a)", "xxx_\\1", "virginica", perl = TRUE)
month.name[grep("a", month.name)]
library(xml2)
library(stringr)
library(plyr)
library(lubridate)
install.packages("xml2")
library(xml2)
periodos <- expand.grid(anno = 2010:2017, mes = 1:12)
periodos$ind <- periodos$anno * 100 + periodos$mes
periodos <- periodos[periodos$ind < 201711,]
periodos <- paste(periodos$anno, str_pad(periodos$mes, 2, pad = "0"), sep = "_")
raw <- lapply(periodos, function(x){
url <- paste0("http://www.eldiario.es/sitemap_contents_", x, ".xml")
print(url)
as_list(read_xml(url))
})
for (i in 1:10){
x <- paste("value i = ", i)
}
for (i in 1:10){
x <- paste("value i = ", i)
print(x)
}
x <- paste0("value i = ", i)
for (i in 1:10){
x <- paste0("value i = ", i)
print(x)
}
for (i in 1:10){
#x <- paste0("value i = ", i)
print(i)
}
for (i in 1:10){
#x <- paste0("value i = ", i)
print("VALUE" + i)
}
for (i in 1:10){
x <- paste0("value i = ", i)
print(X)
}
for (i in 1:10){
x <- paste0("value i = ", i)
print(X)
}
for (i in 1:10){
x <- paste0("value i = ", i)
print(x)
}
mean(rnorm(10))
dt <- rnorm(10)
dt
stats::mean(dt)
stats::median(dt)
stats::var(dt)
?mean
?mean
mean(dt)
z.test <- function(x,n,p=NULL,conf.level=0.95,alternative="less") {
ts.z <- NULL
cint <- NULL
p.val <- NULL
phat <- x/n
qhat <- 1 - phat
# If you have p0 from the population or H0, use it.
# Otherwise, use phat and qhat to find SE.phat:
if(length(p) > 0) {
q <- 1-p
SE.phat <- sqrt((p*q)/n)
ts.z <- (phat - p)/SE.phat
p.val <- pnorm(ts.z)
if(alternative=="two.sided") {
p.val <- p.val * 2
}
if(alternative=="greater") {
p.val <- 1 - p.val
}
} else {
# If all you have is your sample, use phat to find
# SE.phat, and don't run the hypothesis test:
SE.phat <- sqrt((phat*qhat)/n)
}
cint <- phat + c(
-1*((qnorm(((1 - conf.level)/2) + conf.level))*SE.phat),
((qnorm(((1 - conf.level)/2) + conf.level))*SE.phat) )
return(list(estimate=phat,ts.z=ts.z,p.val=p.val,cint=cint))
}
z.test(1,20)
z.test(1,20,p=0.1)
install.packages("packagefinder")
library(packagefinder)
findPackage("regression")
findPackage("pdf")
findPackage("pdf", index =  buildIndex())
setwd("~/")
setwd("~/Google Drive/presentacion_opendataes")
knitr::opts_chunk$set(echo = FALSE)
remotes::install_github("cimentadaj/opendataes")
install.packages("remotes")
remotes::install_github("cimentadaj/opendataes")
library(opendataes)
pb_code <- publishers_available$publisher_code[publishers_available$publishers == 'Ayuntamiento de Barcelona']
pb_code
kw <- explorar_keywords('elecciones', pb_code)
kw <- explorar_keywords('elecciones', pb_code)
kw
final_dt <- kw[grepl("Elecciones al Parlamento Europeo. % sobre electores", kw$description), ]
elections <- cargar_datos(final_dt, encoding = 'latin1')
elections
class(elections)
typeof(elections)
mode(elections)
elections$data
elections$data$`2014_europees-a02.csv`
elections$data$`2014_europees-a02.csv`
elections$data$`2014_europees-a02.csv`
elections$data$
elections$data
elections$data
elections$metadata
elections$metadata
elections$metadata
elections$data
remotes::install_github("cimentadaj/opendataes")
library(opendataes)
pb_code <- publishers_available$publisher_code[publishers_available$publishers
== 'Ayuntamiento de Barcelona']
kw <- explorar_keywords('elecciones', pb_code)
final_dt <- kw[grepl("Elecciones al Parlamento Europeo. % sobre electores", kw$description), ]
elections <- cargar_datos(final_dt, encoding = 'latin1')
elections$data
elections <- cargar_datos(final_dt)
elections
elections$data
elections <- cargar_datos(final_dt, encoding = "UTF8")
elections$data
elections$data$`2014_europees-a02.csv`
remotes::install_github("cimentadaj/opendataes")
remotes::install_github("cimentadaj/opendataes", force = T)
library(opendataes)
pb_code <- publishers_available$publisher_code[publishers_available$publishers
== 'Ayuntamiento de Barcelona']
kw <- explorar_keywords('elecciones', pb_code)
head(kw, 3)
head(kw, 3)
View(kw)
kw <- explorar_keywords('elecciones', pb_code)
kw
View(kw)
library(opendataes)
pb_code <- publishers_available$publisher_code[publishers_available$publishers
== 'Ayuntamiento de Barcelona']
kw <- explorar_keywords('elecciones', pb_code)
View(kw)
kw <- explorar_keywords('elecciones', pb_code)
View(kw)
View(kw)
kw <- explorar_keywords('elecciones', pb_code)
View(kw)
getwd()
